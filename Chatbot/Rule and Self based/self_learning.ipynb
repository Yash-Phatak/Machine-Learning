{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('chatbot.txt','r',errors='ignore')\n",
    "raw_doc = f.read()\n",
    "raw_doc = raw_doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Yash\n",
      "[nltk_data]     Phatak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Yash\n",
      "[nltk_data]     Phatak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Yash\n",
      "[nltk_data]     Phatak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') #tokenizer\n",
    "nltk.download('wordnet') #wordnet dictionary\n",
    "nltk.download('omw-1.4')\n",
    "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
    "while None in sentence_tokens:\n",
    "    sentence_tokens.remove(None)\n",
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chatbot\\n\\narticle\\ntalk\\nread\\nview source\\nview history\\n\\ntools\\npage semi-protected\\nfrom wikipedia, the free encyclopedia\\nfor other uses, see chatbot (disambiguation).',\n",
       " 'parts of this article (those related to everything, particularly sections after the intro) need to be updated.',\n",
       " 'the reason given is: this article is using citations from 1970 and virtually all claims about conversational capabilities are at least ten years out of date (for example the turing test was arguably made obsolete years ago by transformer models).',\n",
       " 'please help update this article to reflect recent events or newly available information.',\n",
       " '(february 2023)\\n\\na virtual assistant chatbot\\n\\nthe 1966 eliza chatbot\\na chatbot (originally chatterbot[1]) is a software application that aims to mimic human conversation through text or voice interactions, typically online.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chatbot', 'article', 'talk', 'read', 'view']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    for token in tokens:\n",
    "        lemmer.lemmatize(token)\n",
    "remove_punct = dict((ord(punct),None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chatbot\\n\\narticle\\ntalk\\nread\\nview source\\nview history\\n\\ntools\\npage semi-protected\\nfrom wikipedia, the free encyclopedia\\nfor other uses, see chatbot (disambiguation).',\n",
       " 'parts of this article (those related to everything, particularly sections after the intro) need to be updated.',\n",
       " 'the reason given is: this article is using citations from 1970 and virtually all claims about conversational capabilities are at least ten years out of date (for example the turing test was arguably made obsolete years ago by transformer models).',\n",
       " 'please help update this article to reflect recent events or newly available information.',\n",
       " '(february 2023)\\n\\na virtual assistant chatbot\\n\\nthe 1966 eliza chatbot\\na chatbot (originally chatterbot[1]) is a software application that aims to mimic human conversation through text or voice interactions, typically online.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greeting \n",
    "greet_inputs = (\"hello\",\"hi\",\"hey\",\"how are you\",\"yo!\")\n",
    "greet_responses = (\"Hellooo\",\"Hellos\",\"Heylo!\",\"How you doin!\",\"Wassup!\",\"There there!\",\"Hi!\",\"Yo!\")\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Response Generation by the Bot\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Response\n",
    "def response(user_response):\n",
    "    sentence_tokens.append(user_response)  # Change 'user' to 'user_response'\n",
    "    robo1_response = ''\n",
    "    TFIDF = TfidfVectorizer(tokenizer=LemNormalize,stop_words='english') #object\n",
    "    tfidf = TFIDF.fit_transform(sentence_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if req_tfidf==0:\n",
    "        robo1_response=robo1_response+\"I am Sorry. Unable to Understand you :(\"\n",
    "        return robo1_response\n",
    "    else:\n",
    "        robo1_response= robo1_response+sentence_tokens[idx]\n",
    "        return robo1_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am the Retrieval Learning Bot. Start typing your text after greeting to talk with me:)\n",
      "\n",
      "Bot: "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[39m# sentence_tokens.append(user_response)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m             \u001b[39m# word_tokens = word_tokens+ nltk.word_tokenize(user_response)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m             \u001b[39m# final_words = list(set(word_tokens))\u001b[39;00m\n\u001b[0;32m     21\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBot: \u001b[39m\u001b[39m\"\u001b[39m,end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m             \u001b[39mprint\u001b[39m(response(user_response))\n\u001b[0;32m     23\u001b[0m             sentence_tokens\u001b[39m.\u001b[39mremove(user_response) \u001b[39m#To ensure the corpus is not corrupted/polluted\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[34], line 6\u001b[0m, in \u001b[0;36mresponse\u001b[1;34m(user_response)\u001b[0m\n\u001b[0;32m      4\u001b[0m robo1_response \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      5\u001b[0m TFIDF \u001b[39m=\u001b[39m TfidfVectorizer(tokenizer\u001b[39m=\u001b[39mLemNormalize,stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m#object\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m tfidf \u001b[39m=\u001b[39m TFIDF\u001b[39m.\u001b[39;49mfit_transform(sentence_tokens)\n\u001b[0;32m      7\u001b[0m vals \u001b[39m=\u001b[39m cosine_similarity(tfidf[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],tfidf)\n\u001b[0;32m      8\u001b[0m idx \u001b[39m=\u001b[39m vals\u001b[39m.\u001b[39margsort()[\u001b[39m0\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Yash Phatak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2133\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2127\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2128\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2129\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2130\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2131\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2132\u001b[0m )\n\u001b[1;32m-> 2133\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2135\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yash Phatak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Yash Phatak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1276\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\Yash Phatak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:116\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 116\u001b[0m         doc \u001b[39m=\u001b[39m ngrams(doc, stop_words)\n\u001b[0;32m    117\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m         doc \u001b[39m=\u001b[39m ngrams(doc)\n",
      "File \u001b[1;32mc:\\Users\\Yash Phatak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:249\u001b[0m, in \u001b[0;36m_VectorizerMixin._word_ngrams\u001b[1;34m(self, tokens, stop_words)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39m# handle stop words\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     tokens \u001b[39m=\u001b[39m [w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m w \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m    251\u001b[0m \u001b[39m# handle token n-grams\u001b[39;00m\n\u001b[0;32m    252\u001b[0m min_n, max_n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_range\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "#Defining the ChatFlow\n",
    "flag = True\n",
    "exit_commands = (\"quit\",\"pause\",\"exit\",\"bye\",\"goodbye\",\"later\",\"ttyl\",\"see ya\")\n",
    "thank_commands = (\"thanks\",\"thankss\",\"danke\",\"dhanyawad\",\"merci\",\"gracias\",\"thank you\")\n",
    "print(\"Hello! I am the Retrieval Learning Bot. Start typing your text after greeting to talk with me:)\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response not in exit_commands):\n",
    "        if(user_response in thank_commands):\n",
    "            flag=False\n",
    "            print(\"Bot: You are Welcome :)\")\n",
    "        else:\n",
    "            if(greeting(user_response) is not None): \n",
    "                print(\"Bot: \"+greeting(user_response))\n",
    "            else: #self-learning\n",
    "                print(user_response)\n",
    "                # sentence_tokens.append(user_response)\n",
    "                # word_tokens = word_tokens+ nltk.word_tokenize(user_response)\n",
    "                # final_words = list(set(word_tokens))\n",
    "                print(\"Bot: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sentence_tokens.remove(user_response) #To ensure the corpus is not corrupted/polluted\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Bot: Goodbye!Have a nice day!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
